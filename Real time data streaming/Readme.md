This material was created as part of my study on real-time data streaming. I followed this repository(https://github.com/airscholar/e2e-data-engineering) as a reference while learning. The primary purpose of this work is to enhance my understanding of real-time data processing.

This system architecture represents a real-time data pipeline that collects data from an API, processes it, and stores it in a NoSQL database. Apache Airflow manages the data pipeline by orchestrating tasks and scheduling workflows while storing metadata in PostgreSQL. It then sends data to Apache Kafka, a distributed streaming platform, where the data is processed in real-time. Producers generate messages, Brokers manage and store data, and Consumers retrieve it. Kafka integrates with Control Center for monitoring and Schema Registry to ensure data integrity by managing schemas.

Once Kafka streams the data, Apache Spark processes it in a distributed manner using PySpark to clean, transform, and aggregate information. The processed data is then stored in Apache Cassandra, a highly scalable NoSQL database, ensuring fast queries and horizontal scalability. ZooKeeper manages the Kafka cluster, while Docker provides containerized deployment. This pipeline ensures efficient real-time data streaming, processing, and storage for analytics or further API-based usage.
